{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qiy6wKQPPQL"
      },
      "source": [
        "# CLASIFICADOR DE TÓPICOS PARA NOTICIAS PERIODÍSTICAS UTILIZANDO NAIVE BAYES\n",
        "\n",
        "## Integrantes del Grupo 10\n",
        "- Palomino Julian Alex Marcelo - Código: 21200244\n",
        "- Adolfo Paucar Kiltom - Código: 22200244\n",
        "- Calderon Zuñiga Rodrigo Joaquin - Código: 22200075\n",
        "\n",
        "Este proyecto presenta el desarrollo de un sistema inteligente basado en el algoritmo Naive Bayes para clasificar artículos periodísticos en distintos tópicos. Se emplea un dataset representativo de noticias reales, preprocesado y vectorizado para alimentar al modelo. El objetivo es demostrar la capacidad de este clasificador probabilístico en problemas de categorización textual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkKf_2kqQ4o0"
      },
      "source": [
        "## Objetivos\n",
        "\n",
        "### Objetivo General\n",
        "Desarrollar e implementar un clasificador automático de noticias periodísticas basado en el algoritmo Naive Bayes que permita categorizar textos noticiosos de manera eficiente y con alta precisión.\n",
        "\n",
        "### Objetivos Específicos\n",
        "- Implementar el algoritmo Multinomial Naive Bayes para clasificación de texto\n",
        "- Desarrollar un pipeline completo de preprocesamiento de texto en español\n",
        "- Crear una interfaz gráfica intuitiva para la interacción con el sistema\n",
        "- Evaluar el rendimiento del modelo mediante métricas estándar de clasificación\n",
        "- Comparar los resultados con técnicas alternativas de clasificación\n",
        "\n",
        "## Marco Teórico\n",
        "\n",
        "### 1. Procesamiento de Lenguaje Natural (PLN)\n",
        "\n",
        "El **Procesamiento de Lenguaje Natural** es una rama de la inteligencia artificial que se enfoca en la interacción entre computadoras y lenguaje humano. En el contexto de clasificación de texto, el PLN permite transformar datos no estructurados (texto) en representaciones numéricas que los algoritmos de machine learning pueden procesar.\n",
        "\n",
        "### 2. Clasificación de Texto\n",
        "\n",
        "La **clasificación de texto** es una tarea supervisada de machine learning donde el objetivo es asignar etiquetas predefinidas a documentos de texto. En el ámbito periodístico, esto permite organizar automáticamente noticias por categorías como deportes, política, tecnología, etc.\n",
        "\n",
        "### 3. Algoritmo Naive Bayes\n",
        "\n",
        "#### 3.1 Fundamentos Teóricos\n",
        "\n",
        "Naive Bayes es una familia de clasificadores probabilísticos basados en el **Teorema de Bayes** con la suposición \"naive\" (ingenua) de independencia condicional entre las características.\n",
        "\n",
        "**Teorema de Bayes:**\n",
        "```\n",
        "P(C|X) = P(X|C) × P(C) / P(X)\n",
        "```\n",
        "\n",
        "Donde:\n",
        "- `P(C|X)`: Probabilidad posterior de la clase C dado el vector de características X\n",
        "- `P(X|C)`: Verosimilitud - probabilidad de observar X dada la clase C\n",
        "- `P(C)`: Probabilidad a priori de la clase C\n",
        "- `P(X)`: Evidencia - probabilidad marginal de observar X\n",
        "\n",
        "#### 3.2 Suposición de Independencia\n",
        "\n",
        "La suposición \"naive\" establece que todas las características son condicionalmente independientes dada la clase:\n",
        "\n",
        "```\n",
        "P(X|C) = P(x₁|C) × P(x₂|C) × ... × P(xₙ|C) = ∏ᵢ₌₁ⁿ P(xᵢ|C)\n",
        "```\n",
        "\n",
        "Esta simplificación reduce significativamente la complejidad computacional y los requerimientos de datos de entrenamiento.\n",
        "\n",
        "#### 3.3 Multinomial Naive Bayes\n",
        "\n",
        "Para clasificación de texto, se utiliza la variante **Multinomial Naive Bayes**, especialmente diseñada para características que representan conteos discretos (como frecuencias de palabras).\n",
        "\n",
        "**Fórmula de clasificación:**\n",
        "```\n",
        "P(xᵢ|C) = (count(xᵢ, C) + α) / (∑ⱼ count(xⱼ, C) + α|V|)\n",
        "```\n",
        "\n",
        "Donde:\n",
        "- `count(xᵢ, C)`: Número de veces que aparece la característica xᵢ en documentos de clase C\n",
        "- `α`: Parámetro de suavizado de Laplace (típicamente α = 1)\n",
        "- `|V|`: Tamaño del vocabulario\n",
        "\n",
        "### 4. Preprocesamiento de Texto\n",
        "\n",
        "#### 4.1 Pipeline de Preprocesamiento\n",
        "\n",
        "El preprocesamiento es crucial para el éxito de cualquier modelo de PLN. El pipeline incluye:\n",
        "\n",
        "1. **Normalización:** Conversión a minúsculas, eliminación de caracteres especiales\n",
        "2. **Tokenización:** Separación del texto en unidades individuales (tokens)\n",
        "3. **Eliminación de stopwords:** Remoción de palabras comunes sin valor semántico\n",
        "4. **Stemming/Lemmatización:** Reducción de palabras a su forma base\n",
        "5. **Filtrado:** Eliminación de tokens muy frecuentes o muy raros\n",
        "\n",
        "#### 4.2 Vectorización TF-IDF\n",
        "\n",
        "**Term Frequency-Inverse Document Frequency (TF-IDF)** es una técnica de vectorización que pondera la importancia de cada término:\n",
        "\n",
        "```\n",
        "TF-IDF(t,d) = TF(t,d) × IDF(t)\n",
        "```\n",
        "\n",
        "Donde:\n",
        "- `TF(t,d) = count(t,d) / |d|`: Frecuencia relativa del término t en el documento d\n",
        "- `IDF(t) = log(N / df(t))`: Frecuencia inversa del documento\n",
        "- `N`: Número total de documentos\n",
        "- `df(t)`: Número de documentos que contienen el término t\n",
        "\n",
        "Esta representación ayuda a identificar términos distintivos para cada clase.\n",
        "\n",
        "### 5. Dataset: 20 Newsgroups\n",
        "\n",
        "El conjunto de datos **20 Newsgroups** es un corpus estándar que contiene aproximadamente 20,000 mensajes de grupos de noticias organizados en 20 categorías temáticas:\n",
        "\n",
        "- **Categorías incluidas:** comp.graphics, rec.sport.baseball, talk.politics.misc, etc.\n",
        "- **Características:** Textos de longitud variable, vocabulario diverso, múltiples dominios\n",
        "- **Utilidad:** Benchmark estándar para algoritmos de clasificación de texto\n",
        "\n",
        "### 6. Métricas de Evaluación\n",
        "\n",
        "#### 6.1 Métricas Principales\n",
        "\n",
        "- **Accuracy:** Proporción de predicciones correctas\n",
        "- **Precision:** Proporción de verdaderos positivos entre predicciones positivas\n",
        "- **Recall:** Proporción de verdaderos positivos entre casos positivos reales\n",
        "- **F1-Score:** Media harmónica entre precision y recall\n",
        "\n",
        "#### 6.2 Matriz de Confusión\n",
        "\n",
        "Herramienta visual que permite analizar el rendimiento del clasificador por clase, identificando patrones de confusión entre categorías similares.\n",
        "\n",
        "### 7. Comparación con Otras Técnicas\n",
        "\n",
        "| Técnica | Ventajas | Desventajas | Complejidad |\n",
        "|---------|----------|-------------|-------------|\n",
        "| **Naive Bayes** | Rápido, simple, interpretable | Suposición de independencia | O(n×m) |\n",
        "| **SVM** | Efectivo en alta dimensionalidad | Lento en datasets grandes | O(n³) |\n",
        "| **Random Forest** | Robusto, maneja overfitting | Menos interpretable | O(n×log(n)×m) |\n",
        "| **Redes Neuronales** | Muy flexible, alta precisión | Requiere muchos datos, \"caja negra\" | O(n×h×e) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWZ-fI8iSFQk"
      },
      "source": [
        "## Software y Herramientas\n",
        "\n",
        "### Frameworks Utilizados\n",
        "- **scikit-learn:** Implementación de Multinomial Naive Bayes y TF-IDF\n",
        "- **NLTK/spaCy:** Preprocesamiento de texto y recursos lingüísticos  \n",
        "- **pandas/numpy:** Manipulación y procesamiento de datos\n",
        "- **matplotlib/seaborn:** Visualización de resultados\n",
        "- **tkinter:** Desarrollo de interfaz gráfica\n",
        "\n",
        "## Metodología\n",
        "\n",
        "1. **Carga y exploración del dataset**\n",
        "2. **Preprocesamiento y limpieza de datos**\n",
        "3. **División en conjuntos de entrenamiento y prueba**\n",
        "4. **Vectorización TF-IDF**\n",
        "5. **Entrenamiento del modelo Naive Bayes**\n",
        "6. **Evaluación y análisis de resultados**\n",
        "7. **Desarrollo de interfaz gráfica**\n",
        "8. **Validación del sistema completo**\n",
        "\n",
        "## Bibliografía\n",
        "\n",
        "1. Wikipedia. (2023). Naive Bayes classifier. Recuperado de\n",
        "https://en.wikipedia.org/wiki/Naive_Bayes_classifier.\n",
        "2. Wikipedia. (2023). Tfidf. Recuperado de https://en.wikipedia.org/wiki/Tfidf.\n",
        "3. scikit-learn. (2019). 5.6.2. The 20 newsgroups text dataset. Recuperado de\n",
        "https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html.\n",
        "4. GeeksforGeeks. (2025). Naive Bayes Classifiers. Recuperado de\n",
        "https://www.geeksforgeeks.org/machine-learning/naive-bayes-classifiers/.\n",
        "5. Analytics Vidhya. (2025). Naive Bayes Classifier Explained With Practical Problems.\n",
        "Recuperado de https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
